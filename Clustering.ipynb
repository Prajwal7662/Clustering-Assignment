{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n"
      ],
      "metadata": {
        "id": "uBoAMwsCVyfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is unsupervised learning?\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the model learns patterns from unlabeled data.\n",
        "It finds hidden structures, such as clusters or associations, without predefined output labels.\n",
        "\n",
        "2. How does K-Means clustering work?\n",
        "\n",
        "Choose number of clusters K\n",
        "\n",
        "Initialize K centroids (random or K-Means++)\n",
        "\n",
        "Assign each point to the nearest centroid\n",
        "\n",
        "Recalculate centroids based on assigned points\n",
        "\n",
        "Repeat steps 3–4 until centroids stop moving (convergence)\n",
        "\n",
        "3. What is a dendrogram in hierarchical clustering?\n",
        "\n",
        "A dendrogram is a tree-like diagram that shows how data points are merged or split at each step in hierarchical clustering. It helps decide how many clusters to choose.\n",
        "\n",
        "4. Main difference between K-Means and Hierarchical Clustering\n",
        "\n",
        "K-Means: Requires specifying K beforehand and uses centroid-based partitioning.\n",
        "\n",
        "Hierarchical: Creates a tree structure and does not require K initially.\n",
        "\n",
        "5. Advantages of DBSCAN over K-Means\n",
        "\n",
        "Can find arbitrarily shaped clusters\n",
        "\n",
        "Automatically detects noise/outliers\n",
        "\n",
        "No need to specify number of clusters K\n",
        "\n",
        "Works well with clusters of different densities\n",
        "\n",
        "6. When is Silhouette Score used?\n",
        "\n",
        "Used to measure how well data points fit within their cluster and how separated they are from other clusters.\n",
        "Higher score = better clustering quality.\n",
        "\n",
        "7. Limitations of Hierarchical Clustering\n",
        "\n",
        "Computationally expensive (O(n²))\n",
        "\n",
        "Not suitable for very large datasets\n",
        "\n",
        "Once merged/split, clusters cannot be undone\n",
        "\n",
        "Sensitive to noise and scaling\n",
        "\n",
        "8. Why is feature scaling important in K-Means?\n",
        "\n",
        "Because K-Means uses Euclidean distance, features with larger values dominate.\n",
        "Scaling ensures all features contribute equally.\n",
        "\n",
        "9. How does DBSCAN identify noise points?\n",
        "\n",
        "Points that:\n",
        "\n",
        "Have fewer than MinPts neighbors within radius ε,\n",
        "\n",
        "And are not reachable from any core point\n",
        "→ are labeled as noise/outliers.\n",
        "\n",
        "10. Define inertia in K-Means\n",
        "\n",
        "Inertia = sum of squared distances of each point to its assigned centroid.\n",
        "Lower inertia means tighter clusters.\n",
        "\n",
        "11. What is the Elbow Method?\n",
        "\n",
        "A technique to choose the optimal K by plotting inertia vs K.\n",
        "The “elbow point” (where curve bends) indicates the best K.\n",
        "\n",
        "12. Describe “density” in DBSCAN\n",
        "\n",
        "Density = number of points within a given radius (ε).\n",
        "If a region has ≥ MinPts within ε ⇒ high density → forms cluster.\n",
        "\n",
        "13. Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "Yes, but only with special distance measures (e.g., Gower distance).\n",
        "Standard Euclidean distance does not work directly.\n",
        "\n",
        "14. What does a negative Silhouette Score indicate?\n",
        "\n",
        "A negative score means:\n",
        "\n",
        "Data points are closer to other clusters than their own\n",
        "→ Very poor clustering.\n",
        "\n",
        "15. What is linkage criteria in hierarchical clustering?\n",
        "\n",
        "Rule used to decide distance between clusters, such as:\n",
        "\n",
        "Single linkage\n",
        "\n",
        "Complete linkage\n",
        "\n",
        "Average linkage\n",
        "\n",
        "Ward linkage\n",
        "\n",
        "16. Why does K-Means perform poorly with varying cluster sizes/densities?\n",
        "\n",
        "Because it assumes clusters are:\n",
        "\n",
        "Spherical\n",
        "\n",
        "Similar in size and density\n",
        "K-Means fails when clusters differ significantly.\n",
        "\n",
        "17. Core parameters in DBSCAN and their roles\n",
        "\n",
        "ε (epsilon): radius for neighborhood\n",
        "\n",
        "MinPts: minimum points required to form a dense region\n",
        "\n",
        "They define what counts as core, border, or noise.\n",
        "\n",
        "18. How does K-Means++ improve initialization?\n",
        "\n",
        "It selects initial centroids far apart using a probability-based method.\n",
        "Results in:\n",
        "\n",
        "Faster convergence\n",
        "\n",
        "Better clustering quality\n",
        "\n",
        "Reduced chance of bad centroids\n",
        "\n",
        "19. What is agglomerative clustering?\n",
        "\n",
        "A bottom-up hierarchical method:\n",
        "\n",
        "Each point starts as its own cluster\n",
        "\n",
        "Clusters are continuously merged\n",
        "\n",
        "Based on minimum distance defined by linkage criteria\n",
        "\n",
        "20. Why is Silhouette Score better than only inertia?\n",
        "\n",
        "Inertia only measures compactness\n",
        "\n",
        "Silhouette score measures compactness + separation\n",
        "→ Gives a full picture of cluster quality."
      ],
      "metadata": {
        "id": "lF20HYL5V3XL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical\n"
      ],
      "metadata": {
        "id": "P7qa8eQOV7za"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "908IBlTHVuU6"
      },
      "outputs": [],
      "source": []
    }
  ]
}